{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run the mp_setup.sh bash file (I wasnt sure if we wanted to keep using a notebook)\n",
    "# Will install mediapipe to the global env"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#@markdown To better demonstrate the Pose Landmarker API, we have created a set of visualization tools that will be used in this colab. These will draw the landmarks on a detect person, as well as the expected connections between those markers.\n",
    "\n",
    "from mediapipe import solutions\n",
    "from mediapipe.framework.formats import landmark_pb2\n",
    "import numpy as np\n",
    "\n",
    "#(image, pose lankmarks)\n",
    "def draw_landmarks_on_image(rgb_image, detection_result):\n",
    "  \n",
    "  #extract pose landmarks from the detection result\n",
    "  pose_landmarks_list = detection_result.pose_landmarks\n",
    "\n",
    "  #copy input image\n",
    "  annotated_image = np.copy(rgb_image)\n",
    "\n",
    "  # Loop through the detected poses to visualize.\n",
    "  for idx in range(len(pose_landmarks_list)):\n",
    "    pose_landmarks = pose_landmarks_list[idx]\n",
    "\n",
    "    # Draw the pose landmarks.\n",
    "    pose_landmarks_proto = landmark_pb2.NormalizedLandmarkList()\n",
    "    pose_landmarks_proto.landmark.extend([\n",
    "      landmark_pb2.NormalizedLandmark(x=landmark.x, y=landmark.y, z=landmark.z) for landmark in pose_landmarks\n",
    "    ])\n",
    "    solutions.drawing_utils.draw_landmarks(\n",
    "      annotated_image,\n",
    "      pose_landmarks_proto,\n",
    "      solutions.pose.POSE_CONNECTIONS,\n",
    "      solutions.drawing_styles.get_default_pose_landmarks_style())\n",
    "  return annotated_image\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "\n",
    "def cv2_imshow(image):\n",
    "    \"\"\"\n",
    "    Display an image using matplotlib in Jupyter Notebook.\n",
    "\n",
    "    Args:\n",
    "        image: The image to display. Should be in BGR format (OpenCV default).\n",
    "    \"\"\"\n",
    "    # Convert the image from BGR (OpenCV default) to RGB (matplotlib default)\n",
    "    img_rgb = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n",
    "\n",
    "    # Display the image using matplotlib\n",
    "    plt.imshow(img_rgb)\n",
    "    plt.axis('off')  # Hide the axis\n",
    "    plt.show()\n",
    "\n",
    "girl_img = cv2.imread(\"sample-frames/mp_example.png\")\n",
    "cv2_imshow(girl_img)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Media Pipeline**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import mediapipe as mp\n",
    "from mediapipe.tasks import python\n",
    "from mediapipe.tasks.python import vision\n",
    "\n",
    "def mp_init_detector(model_path='mp-model/pose_landmarker.task'):\n",
    "    # STEP 2: Create a PoseLandmarker object.\n",
    "    base_options = python.BaseOptions(model_asset_path=model_path)\n",
    "    options = vision.PoseLandmarkerOptions(\n",
    "        base_options=base_options,\n",
    "        output_segmentation_masks=True)\n",
    "    return vision.PoseLandmarker.create_from_options(options)\n",
    "\n",
    "def mp_process_image(detector, img_path):\n",
    "    image = mp.Image.create_from_file(img_path)\n",
    "\n",
    "    # Detect features in image\n",
    "    detection_result = detector.detect(image)\n",
    "\n",
    "    # Associate enums with string values\n",
    "    body_parts_dict = {\n",
    "        'nose': mp.solutions.pose.PoseLandmark.NOSE,\n",
    "        'left_eye_inner': mp.solutions.pose.PoseLandmark.LEFT_EYE_INNER,\n",
    "        'left_eye': mp.solutions.pose.PoseLandmark.LEFT_EYE,\n",
    "        'left_eye_outer': mp.solutions.pose.PoseLandmark.LEFT_EYE_OUTER,\n",
    "        'right_eye_inner': mp.solutions.pose.PoseLandmark.RIGHT_EYE_INNER,\n",
    "        'right_eye': mp.solutions.pose.PoseLandmark.RIGHT_EYE,\n",
    "        'right_eye_outer': mp.solutions.pose.PoseLandmark.RIGHT_EYE_OUTER,\n",
    "        'left_ear': mp.solutions.pose.PoseLandmark.LEFT_EAR,\n",
    "        'right_ear': mp.solutions.pose.PoseLandmark.RIGHT_EAR,\n",
    "        'mouth_left': mp.solutions.pose.PoseLandmark.MOUTH_LEFT,\n",
    "        'mouth_right': mp.solutions.pose.PoseLandmark.MOUTH_RIGHT,\n",
    "        'left_shoulder': mp.solutions.pose.PoseLandmark.LEFT_SHOULDER,\n",
    "        'right_shoulder': mp.solutions.pose.PoseLandmark.RIGHT_SHOULDER,\n",
    "        'left_elbow': mp.solutions.pose.PoseLandmark.LEFT_ELBOW,\n",
    "        'right_elbow': mp.solutions.pose.PoseLandmark.RIGHT_ELBOW,\n",
    "        'left_wrist': mp.solutions.pose.PoseLandmark.LEFT_WRIST,\n",
    "        'right_wrist': mp.solutions.pose.PoseLandmark.RIGHT_WRIST,\n",
    "        'left_pinky': mp.solutions.pose.PoseLandmark.LEFT_PINKY,\n",
    "        'right_pinky': mp.solutions.pose.PoseLandmark.RIGHT_PINKY,\n",
    "        'left_index': mp.solutions.pose.PoseLandmark.LEFT_INDEX,\n",
    "        'right_index': mp.solutions.pose.PoseLandmark.RIGHT_INDEX,\n",
    "        'left_thumb': mp.solutions.pose.PoseLandmark.LEFT_THUMB,\n",
    "        'right_thumb': mp.solutions.pose.PoseLandmark.RIGHT_THUMB,\n",
    "        'left_hip': mp.solutions.pose.PoseLandmark.LEFT_HIP,\n",
    "        'right_hip': mp.solutions.pose.PoseLandmark.RIGHT_HIP,\n",
    "        'left_knee': mp.solutions.pose.PoseLandmark.LEFT_KNEE,\n",
    "        'right_knee': mp.solutions.pose.PoseLandmark.RIGHT_KNEE,\n",
    "        'left_ankle': mp.solutions.pose.PoseLandmark.LEFT_ANKLE,\n",
    "        'right_ankle': mp.solutions.pose.PoseLandmark.RIGHT_ANKLE,\n",
    "        'left_heel': mp.solutions.pose.PoseLandmark.LEFT_HEEL,\n",
    "        'right_heel': mp.solutions.pose.PoseLandmark.RIGHT_HEEL,\n",
    "        'left_foot_index': mp.solutions.pose.PoseLandmark.LEFT_FOOT_INDEX,\n",
    "        'right_foot_index': mp.solutions.pose.PoseLandmark.RIGHT_FOOT_INDEX\n",
    "    }\n",
    "\n",
    "    # Create a dictonary so we can index into body parts\n",
    "    # EX Usage: body_parts['ankle_left]\n",
    "    body_parts = {}\n",
    "    if detection_result.pose_landmarks:\n",
    "        for pose_landmarks in detection_result.pose_landmarks:\n",
    "            for part_name, part_enum in body_parts_dict.items():\n",
    "                body_parts[part_name] = pose_landmarks[part_enum]\n",
    "\n",
    "    return detection_result, body_parts\n",
    "\n",
    "def mp_debug_show_image(detector, img_path):\n",
    "    image = mp.Image.create_from_file(img_path)\n",
    "\n",
    "    if image is None:\n",
    "        raise ValueError(f\"Image at path {img_path} could not be loaded.\")\n",
    "\n",
    "    detection_result, _ = mp_process_image(detector, img_path)\n",
    "\n",
    "    image_rgb = cv2.cvtColor(cv2.imread(img_path), cv2.COLOR_BGR2RGB)\n",
    "\n",
    "    # STEP 5: Process the detection result. In this case, visualize it.\n",
    "    annotated_image = draw_landmarks_on_image(\n",
    "        image_rgb,\n",
    "        detection_result\n",
    "    )\n",
    "\n",
    "    # Display the annotated image\n",
    "    cv2_imshow(cv2.cvtColor(annotated_image, cv2.COLOR_RGB2BGR))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Usage\n",
    "mp_detector = mp_init_detector()\n",
    "detection_results, body_parts = mp_process_image(\n",
    "    mp_detector,\n",
    "    \"sample-frames/mp_example.png\"\n",
    ")\n",
    "\n",
    "left_ankle = body_parts['left_ankle']\n",
    "print(\"Left Ankle\")\n",
    "print(f\"position: {left_ankle.x:.3f}, {left_ankle.y:.3f}, {left_ankle.z:.3f}\")\n",
    "print(f\"visibility: {left_ankle.visibility:.3f}\")\n",
    "\n",
    "# Debug\n",
    "mp_debug_show_image(mp_detector, \"sample-frames/mp_example.png\")\n",
    "mp_debug_show_image(mp_detector,\"sample-frames/good1.png\")\n",
    "mp_debug_show_image(mp_detector,\"sample-frames/good2.png\")\n",
    "\n",
    "mp_debug_show_image(mp_detector,\"sample-frames/bad1.png\")\n",
    "mp_debug_show_image(mp_detector,\"sample-frames/bad2.png\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Push Up Detection**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "\n",
    "def calculate_angle(a, b, c):\n",
    "    a = np.array([a.x, a.y, a.z])\n",
    "    b = np.array([b.x, b.y, b.z])\n",
    "    c = np.array([c.x, c.y, c.z])\n",
    "\n",
    "    radians = np.arctan2(c[1] - b[1], c[0] - b[0]) - np.arctan2(a[1] - b[1], a[0] - b[0])\n",
    "    angle = np.abs(radians * 180.0 / np.pi)\n",
    "\n",
    "    if angle > 180.0:\n",
    "        angle = 360 - angle\n",
    "\n",
    "    return angle\n",
    "\n",
    "def do_pushup(body_parts, vis_threshold=0.75):\n",
    "    l_sh = body_parts['left_shoulder']\n",
    "    l_elbow = body_parts['left_elbow']\n",
    "    l_wrist = body_parts['left_wrist']\n",
    "    r_sh = body_parts['right_shoulder']\n",
    "    r_elbow = body_parts['right_elbow']\n",
    "    r_wrist = body_parts['right_wrist']\n",
    "    l_hip = body_parts['left_hip']\n",
    "    r_hip = body_parts['right_hip']\n",
    "    l_ankle = body_parts['left_ankle']\n",
    "    r_ankle = body_parts['right_ankle']\n",
    "\n",
    "    # Get visibility of left and right sides\n",
    "    # Left arm\n",
    "    l_vis = l_wrist.visibility + l_elbow.visibility + l_sh.visibility\n",
    "    # Left leg\n",
    "    l_vis += l_ankle.visibility + l_hip.visibility\n",
    "    # Normalize\n",
    "    l_vis = l_vis / 5\n",
    "\n",
    "    # Right arm\n",
    "    r_vis = r_wrist.visibility + r_elbow.visibility + r_sh.visibility\n",
    "    # Right leg\n",
    "    r_vis += r_ankle.visibility + r_hip.visibility\n",
    "    # Normalize\n",
    "    r_vis = r_vis / 5\n",
    "\n",
    "    # Visibility (assuming 1 arm/ankle will always be obstructed)\n",
    "    if l_vis < vis_threshold and r_vis >= vis_threshold:\n",
    "        l_sh, l_elbow, l_wrist = r_sh, r_elbow, r_wrist\n",
    "        l_hip, l_ankle = r_hip, r_ankle\n",
    "    elif r_vis < vis_threshold and l_vis >= vis_threshold:\n",
    "        r_sh, r_elbow, r_wrist = l_sh, l_elbow, l_wrist\n",
    "        r_hip, r_ankle = l_hip, l_ankle\n",
    "    elif l_vis < vis_threshold and r_vis < vis_threshold:\n",
    "        return -1\n",
    "\n",
    "    # Calculate angles\n",
    "    shoulder_angle = calculate_angle(l_hip, l_sh, l_elbow)\n",
    "    elbow_angle = calculate_angle(l_sh, l_elbow, l_wrist)\n",
    "    hip_angle = calculate_angle(l_sh, l_hip, l_ankle)\n",
    "    print(f\"Shoulder Angle: {shoulder_angle}\")\n",
    "    print(f\"Elbow Angle: {elbow_angle}\")\n",
    "    print(f\"Hip Angle: {hip_angle}\")\n",
    "\n",
    "    if 50 < shoulder_angle < 70 and 160 < elbow_angle < 180 and 160 < hip_angle < 180:\n",
    "        # Raised pushup\n",
    "        return 2\n",
    "    elif 0 < shoulder_angle < 20 and 80 < elbow_angle < 100 and 160 < hip_angle < 180:\n",
    "        # Lower pushup\n",
    "        return 1\n",
    "    else:\n",
    "        # Bad posture\n",
    "        return 0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Deadlift Detection**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pushup_detector(img_path):\n",
    "    # Media pipeline\n",
    "    mp_detector = mp_init_detector()\n",
    "    detection_results, body_parts = mp_process_image(\n",
    "        mp_detector,\n",
    "        img_path\n",
    "    )\n",
    "\n",
    "    # Body part usage\n",
    "    if bool(body_parts):\n",
    "        # Mediapipe debug\n",
    "        mp_debug_show_image(mp_detector, img_path)\n",
    "\n",
    "        # Pushup detect Usage\n",
    "        print(\"Pushup Detector\")\n",
    "        pushup_res  = do_pushup(body_parts, vis_threshold=0.75)\n",
    "        if pushup_res < 0:\n",
    "            print(f\"WARNING: Low visibility confidence, skipping {img_path}\")\n",
    "        elif pushup_res > 0:\n",
    "            print(\"Good posture\")\n",
    "        else:\n",
    "            print(\"Bad posture\")\n",
    "    else:\n",
    "        print(f\"WARNING: Mediapipe could not detect landmarks, skipping {img_path}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Good examples for range of motion\n",
    "pushup_detector(\"sample-frames/pushup_rom1.png\")\n",
    "pushup_detector(\"sample-frames/pushup_rom2.png\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Good examples for bad posture\n",
    "pushup_detector(\"sample-frames/bad_pushup.png\")\n",
    "pushup_detector(\"sample-frames/pushup_bad.jpeg\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Multi angle for good posture\n",
    "pushup_detector(\"sample-frames/pushup2.png\")\n",
    "pushup_detector(\"sample-frames/pushup_good.jpeg\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
